# ü§ñ Evaluator Package

Syst√®me d'√©valuation automatis√© des contributions bas√© sur des agents IA (OpenAI). Ce package identifie, √©value et attribue des r√©compenses aux contributions des d√©veloppeurs.

## üéØ Vue d'ensemble

Le **evaluator** est un syst√®me intelligent qui analyse les contributions (commits, code, datasets, mod√®les, documentation) et les √©value selon des grilles de crit√®res pr√©d√©finies. Il utilise des agents OpenAI pour automatiser le processus d'√©valuation.

## üèóÔ∏è Architecture

```
evaluator/
‚îú‚îÄ‚îÄ evaluator.ts           # Impl√©mentation principale (OpenAIAgentEvaluator)
‚îú‚îÄ‚îÄ interfaces.ts          # Interface AgentEvaluator
‚îú‚îÄ‚îÄ types.ts               # Types de donn√©es (Contribution, Evaluation, etc.)
‚îú‚îÄ‚îÄ reward.ts              # Calcul de distribution des r√©compenses
‚îú‚îÄ‚îÄ grids/                 # Grilles d'√©valuation par type
‚îÇ   ‚îú‚îÄ‚îÄ index.ts           # Registry des grilles
‚îÇ   ‚îú‚îÄ‚îÄ code.grid.ts       # Grille pour le code
‚îÇ   ‚îú‚îÄ‚îÄ model.grid.ts      # Grille pour les mod√®les ML
‚îÇ   ‚îú‚îÄ‚îÄ dataset.grid.ts    # Grille pour les datasets
‚îÇ   ‚îî‚îÄ‚îÄ docs.grid.ts       # Grille pour la documentation
‚îî‚îÄ‚îÄ openai/                # Agents OpenAI
    ‚îú‚îÄ‚îÄ identify.agent.ts  # Agent d'identification
    ‚îî‚îÄ‚îÄ evaluate.agent.ts  # Agent d'√©valuation
```

## üîÑ Processus d'√©valuation

Le syst√®me suit un pipeline en 3 √©tapes :

### 1. **Identification** (`identify`)
Analyse le contexte (r√©unions, commits GitHub) pour identifier les contributions pertinentes.

**Entr√©e** : Contexte (r√©sum√© de r√©union + commits + membres d'√©quipe)  
**Sortie** : Liste de `Contribution[]`

### 2. **√âvaluation** (`evaluate`)
√âvalue chaque contribution selon une grille de crit√®res sp√©cifique √† son type.

**Entr√©e** : `Contribution` + contexte (snapshot du code + grille d'√©valuation)  
**Sortie** : `Evaluation` avec scores d√©taill√©s

### 3. **Agr√©gation** (`aggregate`)
Calcule le score final pond√©r√© √† partir des scores individuels.

**Entr√©e** : `Evaluation`  
**Sortie** : Score global (0-100)

## üìä Types de donn√©es

### **Contribution**
Repr√©sente une contribution identifi√©e √† √©valuer.

```typescript
interface Contribution {
  title: string;           // Titre de la contribution
  type: string;            // Type: "code" | "model" | "dataset" | "docs"
  description?: string;    // Description d√©taill√©e
  challenge_id: string;    // ID du challenge associ√©
  tags?: string[];         // Tags techniques (NextJS, MONAI, etc.)
  userId: string;          // ID de l'utilisateur
  commitSha: string;       // SHA du commit GitHub
}
```

### **CriterionScore**
Score individuel sur un crit√®re d'√©valuation.

```typescript
interface CriterionScore {
  criterion: string;       // Nom du crit√®re (qualit√©, impact, etc.)
  score: number;           // Score de 0 √† 100
  weight: number;          // Poids du crit√®re (0.0 √† 1.0)
  comment?: string;        // Justification du score
}
```

### **Evaluation**
R√©sultat complet de l'√©valuation d'une contribution.

```typescript
interface Evaluation {
  scores: CriterionScore[];       // Scores par crit√®re
  globalScore: number;            // Score global pond√©r√©
  contribution?: Contribution;    // Contribution √©valu√©e
}
```

### **ContributionReward**
R√©compense attribu√©e √† une contribution.

```typescript
interface ContributionReward {
  userId: string;
  contributionTitle: string;
  score: number;           // Score obtenu
  reward: number;          // Contribution Points (CP) attribu√©s
}
```

## üéì Grilles d'√©valuation

Les grilles d√©finissent les crit√®res et leur pond√©ration pour chaque type de contribution.

### **Code Grid** (`code`)

```typescript
{
  type: "code",
  criteriaTemplate: [
    { criterion: "qualit√©", weight: 0.2 },
    { criterion: "impact", weight: 0.3 },
    { criterion: "complexit√©", weight: 0.3 },
    { criterion: "architecture", weight: 0.2 }
  ]
}
```

**Crit√®res** :
- **Qualit√©** (20%) : Propret√©, lisibilit√©, conventions
- **Impact** (30%) : Importance fonctionnelle, valeur ajout√©e
- **Complexit√©** (30%) : Difficult√© technique, innovation
- **Architecture** (20%) : Design patterns, scalabilit√©

### **Model Grid** (`model`)
Grille pour les mod√®les de machine learning.

### **Dataset Grid** (`dataset`)
Grille pour les datasets et donn√©es.

### **Docs Grid** (`docs`)
Grille pour la documentation.

### **Registry des grilles**

```typescript
import { EvaluationGridRegistry } from "@mytwin/evaluator";

// R√©cup√©rer une grille
const grid = EvaluationGridRegistry.getGrid("code");

// V√©rifier l'existence
if (EvaluationGridRegistry.hasGrid("model")) { ... }

// Lister les types disponibles
const types = EvaluationGridRegistry.getAvailableTypes();
// ["code", "model", "dataset", "docs"]
```

## ü§ñ Agents OpenAI

### **Identify Agent**
Agent qui identifie les contributions √† partir d'un contexte.

**Fonctionnement** :
1. Re√ßoit un contexte (r√©sum√© de r√©union + commits + √©quipe)
2. Analyse et extrait les contributions pertinentes
3. √âvite les doublons
4. Lie chaque contribution √† un commit GitHub
5. Retourne un tableau JSON de contributions

**Contraintes** :
- Types autoris√©s : `"code"`, `"model"`, `"dataset"`, `"docs"`
- Obligation de lier chaque contribution √† un commit
- D√©tection automatique des doublons

### **Evaluate Agent**
Agent qui √©value une contribution selon une grille de crit√®res.

**Fonctionnement** :
1. Re√ßoit une contribution + snapshot du code + grille d'√©valuation
2. Utilise l'outil `read_file` pour lire les fichiers n√©cessaires
3. Analyse le code selon les crit√®res de la grille
4. Attribue un score (0-100) et un commentaire par crit√®re
5. Retourne une √©valuation structur√©e

**Outil disponible** : `read_file(path)`
- Permet √† l'agent de lire des fichiers dans le workspace du commit
- Utilis√© pour analyser le code en profondeur

**Boucle de tool calls** :
L'agent peut faire plusieurs appels √† `read_file` avant de produire son √©valuation finale.

## üí∞ Calcul des r√©compenses

La fonction `computeRewards` distribue un pool de Contribution Points (CP) proportionnellement aux scores.

### Algorithme

```typescript
import { computeRewards } from "@mytwin/evaluator";

const rewards = computeRewards(evaluations, totalRewardPool);
```

**√âtapes** :
1. Calcul du score total : `Œ£(globalScore)`
2. Calcul de la proportion de chaque contribution : `score / totalScore`
3. Attribution proportionnelle : `reward = proportion √ó totalRewardPool`
4. Arrondi et ajustement pour garantir la distribution exacte du pool

**Cas particuliers** :
- Si `totalScore = 0` ‚Üí distribution √©gale
- Ajustement d'arrondi sur la derni√®re contribution

### Exemple

```typescript
const evaluations = [
  { globalScore: 80, contribution: { userId: "user1", title: "Feature A" } },
  { globalScore: 60, contribution: { userId: "user2", title: "Feature B" } },
  { globalScore: 40, contribution: { userId: "user3", title: "Feature C" } }
];

const rewards = computeRewards(evaluations, 1000);
// [
//   { userId: "user1", score: 80, reward: 444 },  // 80/180 √ó 1000
//   { userId: "user2", score: 60, reward: 333 },  // 60/180 √ó 1000
//   { userId: "user3", score: 40, reward: 223 }   // 40/180 √ó 1000 + ajustement
// ]
```

## üöÄ Utilisation

### Exemple complet

```typescript
import { OpenAIAgentEvaluator, EvaluationGridRegistry, computeRewards } from "@mytwin/evaluator";

const evaluator = new OpenAIAgentEvaluator();

// 1. Identifier les contributions
const context = {
  meetingSummary: "...",
  commits: [...],
  teamMembers: [...]
};

const contributions = await evaluator.identify(context);

// 2. √âvaluer chaque contribution
const evaluations = [];
for (const contribution of contributions) {
  const grid = EvaluationGridRegistry.getGrid(contribution.type);
  const snapshot = { workspacePath: "/tmp/commit-abc123", ... };
  
  const evaluation = await evaluator.evaluate(contribution, { snapshot, grid });
  evaluation.contribution = contribution;
  evaluations.push(evaluation);
}

// 3. Calculer les scores globaux
for (const evaluation of evaluations) {
  const globalScore = evaluator.aggregate(evaluation);
  console.log(`Score: ${globalScore}/100`);
}

// 4. Distribuer les r√©compenses
const totalRewardPool = 1000; // CP disponibles pour ce challenge
const rewards = computeRewards(evaluations, totalRewardPool);

console.log(rewards);
// [
//   { userId: "user1", contributionTitle: "...", score: 85, reward: 425 },
//   { userId: "user2", contributionTitle: "...", score: 70, reward: 350 },
//   ...
// ]
```

## üîß Interface `AgentEvaluator`

Interface conceptuelle pour tout syst√®me d'√©valuation.

```typescript
interface AgentEvaluator {
  identify(context: any): Promise<Contribution[]>;
  evaluate(contribution: Contribution, context: any): Promise<Evaluation>;
  aggregate(evaluation: Evaluation): number;
}
```

**Impl√©mentation actuelle** : `OpenAIAgentEvaluator`

Permet de cr√©er d'autres impl√©mentations (Claude, Gemini, r√®gles manuelles, etc.).

## ‚öôÔ∏è Configuration

### Variables d'environnement

```env
OPENAI_API_KEY=sk-...
```

### Mod√®le utilis√©

Actuellement : **`gpt-5-nano`** (peut √™tre chang√© pour `gpt-4o-mini` ou autres)

## üìù Bonnes pratiques

1. **Toujours lier une contribution √† un commit** : Permet la tra√ßabilit√©
2. **Utiliser les grilles appropri√©es** : Chaque type a ses crit√®res sp√©cifiques
3. **Fournir un contexte riche** : Plus le contexte est d√©taill√©, meilleure est l'√©valuation
4. **V√©rifier les scores** : Les scores sont entre 0 et 100
5. **Ajuster les poids** : Modifier les grilles selon les besoins du projet

## üéØ Points cl√©s

- ‚úÖ **Pipeline automatis√©** : Identification ‚Üí √âvaluation ‚Üí R√©compenses
- ‚úÖ **Grilles personnalisables** : Crit√®res et poids adaptables par type
- ‚úÖ **Agents IA** : Utilisation d'OpenAI pour l'analyse intelligente
- ‚úÖ **Tool calling** : L'agent peut lire des fichiers pour analyser le code
- ‚úÖ **Distribution proportionnelle** : R√©compenses bas√©es sur les scores
- ‚úÖ **Type-safe** : TypeScript pour la s√©curit√© des types
- ‚úÖ **Extensible** : Interface permettant d'autres impl√©mentations

## üîÆ Extensions possibles

- Ajouter de nouvelles grilles (tests, design, etc.)
- Impl√©menter d'autres agents (Claude, Gemini)
- Ajouter des m√©triques de qualit√© automatiques (coverage, linting)
- Int√©grer des outils d'analyse statique
- Cr√©er un syst√®me de feedback pour am√©liorer les √©valuations
